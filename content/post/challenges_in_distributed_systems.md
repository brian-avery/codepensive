---
layout:     post 
title:      "Challenges in Distributed Systems"
subtitle:   "Processing a Message Once and Only Once"
date:       2017-10-30
author:     "Brian Avery"
URL: "/2017/10/30/challenges-in-distributed-systems/"
image:      "https://img.zhaohuabing.com/post-bg-2015.jpg"
categories:  ["Containers", "Microservices" ]
---

Distributed services are messy. You have network issues, errors in processing, and a range of other problems that occur. Take the problems of a traditional program and increase them with the number of processes. Now think of communication between these problems. Now you're starting to understand some of the challenges in distributed programming. But, these problems are precisely what makes distributed programming so fun.

Take one of the most common problems. You have a request come in to your program. Lets say you're changing a user's name. Seems easy enough right? You submit the request to the API gateway sitting on the edge of your system. This then processes the message before handing it off to the next and until you eventually reach the database layer where you insert the data. You have multiple methods of passing the message. You can use an HTTP request, you can use a message queue such as Kafka, you can use a websocket... but they all have their own problems. With an HTTP request or a websocket, you are communicating between two services. If the first service goes down before sending the message to the next, you lose the message. If you use Kafka, you can replicate the message between multiple nodes of the cluster and fix the problem of the service going down. But not you have a different problem. Each message has to be marked as completed. If the receiver marks the message as completed and then starts processing it, then you've again lost the message if that service restarts. You could process the message and then mark it as complete but in the mean time, another service could walk in and process it as well. Now you have two rows in your database for the one record you wanted to create. Still not an exactly one solution.

Thinking about ways to solve this, you could come up with a solution such as a system involving a lock and a timeout. Read this message off into a system that attaches a lock to each message. The process to read the message locks the file. If the process doesn't respond in a certain period of time, unlock it and the next service to come in can pick up that message and try again. The first service to process to process successfully marks it as complete and returns. This sounds like a better solution right? Well now we have two problems. First, if a service restarts in the middle of processing a message then you may now have incomplete changes in your system (completion of some steps was successful but not others). Second, if there are network issues, the message could be processed correctly but the service that has completed it was not able to mark it as complete before the timeout was hit. This would mean that the next service to execute would come in and execute the message a second time.

The other two approaches here are at most once and at least once. In an at most once system, the message must be processed at most one time. This can lead to data loss but will guarantee that a message is not processed more than once. For example, you only submit your online order once. Load all of the messages onto Kafka  and the first one to read it off marks it complete. If it gets successfully processed, great. If it fails to be processed, that's ok. We've only made the request once and can often notify the user that there was an issue. This is also easy to solve using HTTP requests.

In an at least once system, we can again use Kafka. The first service to read the message off picks it up and processes it, and either completes it successfully and marks it complete or fails. Another service can come in and see that the message has not been completed yet and process it. In this system if any service fails then the next service can come in, process it, and complete it. This guarantees that it's processed but can cause duplicate data in the network.

Message processing in distributed systems is just one example of a problem in distributed systems. We've traded issues with large, monolithic programs that are challenging to debug because of size with small micro programs that are hard to debug because of communication between services. We've introduced problems processing messages a guaranteed number of times. Although, some of these problems like the chance of losing a message exist in monolithic applications, micro services compound these problems as more and more services are introduced.   There are a lot of challenges in distributed systems, but I think that's what makes this industry so much fun to work in.